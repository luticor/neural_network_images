{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset \n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = Image.open('images\\\\megumin_explosion.jpg').convert('L')\n",
    "pixels = img.getdata()\n",
    "\n",
    "width, height = img.size\n",
    "\n",
    "x_coords = []\n",
    "y_coords = []\n",
    "intensity = []\n",
    "for i in range(height):\n",
    "    for j in range(width):\n",
    "        x_coords.append(j)\n",
    "        y_coords.append(height-i)\n",
    "        intensity.append(pixels[i*width + j])\n",
    "\n",
    "df = pd.DataFrame({'x': np.array(x_coords)/width, 'y': np.array(y_coords)/height, 'z': np.array(intensity)/255.})\n",
    "\n",
    "x = np.array(x_coords).reshape((height, width))\n",
    "y = np.array(y_coords).reshape((height, width))\n",
    "z = np.array(intensity).reshape((height, width))\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(img, cmap='Greys_r')\n",
    "plt.show()\n",
    "# plt.imshow(z, cmap='Greys_r')\n",
    "# plt.show()\n",
    "\n",
    "x_tensor = torch.tensor(df.iloc[:,0:2].values, dtype=torch.float)\n",
    "y_tensor = torch.tensor(df.iloc[:,-1].values, dtype=torch.float)\n",
    "\n",
    "# Create an instance of the dataset and dataloader\n",
    "batch_size = 1024 #1024*16\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "print('Batches per Epoch:', df.index[-1]/batch_size)\n",
    "\n",
    "print(x_tensor.shape)\n",
    "print(y_tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'colab_base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n colab_base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, nn_shape=(2, 4, 100, 1)):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_inputs = nn_shape[0]\n",
    "        self.num_layers = nn_shape[1]\n",
    "        self.num_neurons = nn_shape[2]\n",
    "        self.num_outputs = nn_shape[3]\n",
    "        self.layers = nn.ModuleList() # create an empty list to store layers\n",
    "        self.layers.append(nn.Linear(self.num_inputs, self.num_neurons)) # add the first layer\n",
    "        \n",
    "        for i in range(self.num_layers - 2): # add the remaining layers\n",
    "            self.layers.append(nn.Linear(self.num_neurons, self.num_neurons))\n",
    "        \n",
    "        self.layers.append(nn.Linear(self.num_neurons, self.num_outputs)) # add the output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "class AttentionNet(nn.Module):\n",
    "    def __init__(self, nn_shape=(10,3,50,10), cond_inputs = 2, overlap=100):\n",
    "        super(AttentionNet, self).__init__()\n",
    "        #nn_shape = (n_inputs, n_layers, n_neurons, n_outputs)\n",
    "        self.cond_inputs = cond_inputs\n",
    "        self.num_inputs = nn_shape[0]\n",
    "        self.num_layers = nn_shape[1]\n",
    "        self.num_neurons = nn_shape[2]\n",
    "        self.num_outputs = nn_shape[3]\n",
    "        self.overlap = overlap\n",
    "        self.qnet = Net(nn_shape=(self.cond_inputs,self.num_layers, self.num_neurons, self.num_inputs*self.overlap))\n",
    "        self.knet = Net(nn_shape=(self.cond_inputs,self.num_layers, self.num_neurons, self.overlap*self.num_outputs))\n",
    "        self.bnet = Net(nn_shape=(self.cond_inputs,self.num_layers, self.num_neurons, self.num_outputs))\n",
    "\n",
    "    def calc_params(self, x):\n",
    "        q = self.qnet(x)\n",
    "        k = self.knet(x)\n",
    "        q = torch.reshape(q, shape=(-1, self.num_inputs, self.overlap))\n",
    "        k = torch.reshape(k, shape=(-1, self.overlap, self.num_outputs))\n",
    "        w = torch.matmul(q,k)\n",
    "        b = self.bnet(x)\n",
    "\n",
    "        # print('q', q.size())\n",
    "        # print('k', k.size())\n",
    "        # print('w', w.size())\n",
    "        # print('b', b.size())\n",
    "        return w, b\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        '''\n",
    "        Passes in the layer input, x, and the data conditioning input, y, and\n",
    "        then calculates layer weights and biases and uses them to compute layer outputs.\n",
    "        '''\n",
    "        # print('x', x.size())\n",
    "        w, b = self.calc_params(y)\n",
    "        # x = torch.matmul(w,x) + b\n",
    "        x = torch.unsqueeze(x,dim=1)\n",
    "        g = torch.matmul(x,w)\n",
    "        g = torch.squeeze(g, dim=1)\n",
    "        x = g + b\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class NeuralNetNet(nn.Module):\n",
    "    def __init__(self, nn_shape=(2,4,100,1), sub_net_shape=(3,100), overlap = 50):\n",
    "        super(NeuralNetNet, self).__init__()\n",
    "        self.num_inputs = nn_shape[0]\n",
    "        self.num_layers = nn_shape[1]\n",
    "        self.num_neurons = nn_shape[2]\n",
    "        self.num_outputs = nn_shape[3]\n",
    "        self.subnet_layers = sub_net_shape[0]\n",
    "        self.subnet_neurons = sub_net_shape[1]\n",
    "        self.overlap = overlap #number of \"heads\", this is the dimension eaten by the matrix mutliplication of keys and values\n",
    "\n",
    "        self.sub_nets = nn.ModuleList() # create an empty list to store subnets\n",
    "        self.sub_nets.append(AttentionNet(nn_shape=(self.num_inputs, self.subnet_layers, self.subnet_neurons, self.num_neurons),\n",
    "                                          cond_inputs=self.num_inputs, overlap=self.overlap))\n",
    "        for i in range(self.num_layers - 2): # add the remaining layers\n",
    "            self.sub_nets.append(AttentionNet(nn_shape=(self.num_neurons, self.subnet_layers, self.subnet_neurons, self.num_neurons),\n",
    "                                              cond_inputs=self.num_inputs, overlap=self.overlap))\n",
    "        self.sub_nets.append(AttentionNet(nn_shape=(self.num_neurons, self.subnet_layers, self.subnet_neurons, self.num_outputs),\n",
    "                                          cond_inputs=self.num_inputs, overlap=self.overlap))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for sub_net in self.sub_nets[:-1]:\n",
    "            x = F.relu(sub_net(x, y))\n",
    "        x = self.sub_nets[-1](x, y)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create an instance of the neural network\n",
    "# net = Net(nn_shape=(2,6,400,1))\n",
    "# net = Net(nn_shape=(2,3,100,1))\n",
    "net = NeuralNetNet(nn_shape=(2,4,50,1), sub_net_shape=(3,50), overlap = 50)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "def make_reduced_dataloader(scale=20):\n",
    "    xlin = np.linspace(0, 1, width // scale)\n",
    "    ylin = np.linspace(0, 1, height // scale)\n",
    "    xv, yv = np.meshgrid(xlin, ylin)\n",
    "\n",
    "    xv_tensor = torch.tensor(xv, dtype=torch.float).flatten()\n",
    "    yv_tensor = torch.tensor(yv, dtype=torch.float).flatten()\n",
    "\n",
    "    reduced_tensor = torch.stack((xv_tensor, yv_tensor), dim=-1)\n",
    "    reduced_dataset = TensorDataset(reduced_tensor, torch.zeros(size=reduced_tensor.size()))\n",
    "    reduced_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return reduced_dataloader\n",
    "\n",
    "# reduced_dataloader = make_reduced_dataloader(scale=4)\n",
    "\n",
    "# Plot Raw Network Initialization\n",
    "print('Plotting Raw Network Initialization')\n",
    "def plot_performance(net, scale = 4):\n",
    "    reduced_dataloader = make_reduced_dataloader(scale=scale)\n",
    "    with torch.no_grad():\n",
    "        output_list = []\n",
    "        for x_batch, y_batch in reduced_dataloader:\n",
    "            outputs = net(x_batch)\n",
    "            output_list.append(outputs)\n",
    "        outputs = torch.cat(output_list, dim=0)\n",
    "\n",
    "    outputs = outputs.numpy()\n",
    "    output_grid = outputs.reshape((height//scale, width//scale))\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.imshow(output_grid, cmap='Greys_r', origin='lower')\n",
    "    plt.show()\n",
    "\n",
    "plot_performance(net)\n",
    "\n",
    "# Train the network for 1000 epochs\n",
    "loss_values = []\n",
    "n_epochs = 4\n",
    "print(\"Beginning Training\")\n",
    "for epoch in range(n_epochs):\n",
    "    for g in optimizer.param_groups:\n",
    "        # g['lr'] = (0.001/n_epochs)*(n_epochs - epoch) + 0.00001\n",
    "        g['lr'] = 10**(-3. - 2.*(epoch/(n_epochs-1)))\n",
    "    print('Epoch', epoch, 'of', n_epochs)\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        print('x_batch',x_batch.size())\n",
    "        # Forward pass\n",
    "        outputs = net(x_batch)\n",
    "        loss = criterion(outputs, y_batch.unsqueeze(dim=-1))\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store the loss value\n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "    # Print the loss\n",
    "    print(f\"Loss = {loss_values[-1]}\")\n",
    "    # Show current network progress\n",
    "    if epoch in [0,1,2,4,10,20,40]:\n",
    "        plot_performance(net)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(net.state_dict(), 'model2.pt')\n",
    "\n",
    "\n",
    "\n",
    "# Plot the loss values\n",
    "epochs_values = np.array(range(len(loss_values))) / np.ceil(df.index[-1]/batch_size)\n",
    "plt.plot(epochs_values, loss_values)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'colab_base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n colab_base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(width, height)\n",
    "xlin = np.linspace(0, 1, width // 2)\n",
    "ylin = np.linspace(0, 1, height // 2)\n",
    "xv, yv = np.meshgrid(xlin, ylin)\n",
    "\n",
    "xv_tensor = torch.tensor(xv, dtype=torch.float).flatten()\n",
    "yv_tensor = torch.tensor(yv, dtype=torch.float).flatten()\n",
    "\n",
    "reduced_tensor = torch.stack((xv_tensor, yv_tensor), dim=-1)\n",
    "\n",
    "# Print the loss\n",
    "print(f\"Loss = {loss_values[-1]}\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = net(x_tensor)\n",
    "# outputs = outputs.numpy()\n",
    "# output_grid = outputs.reshape((height, width))\n",
    "# plt.figure(figsize=(12,8))\n",
    "# plt.imshow(output_grid, cmap='Greys_r')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = net(reduced_tensor)\n",
    "outputs = outputs.numpy()\n",
    "output_grid = outputs.reshape((height//2, width//2))\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(output_grid, cmap='Greys_r', origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'colab_base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n colab_base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'colab_base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n colab_base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.log10(0.01))\n",
    "print(np.log10(0.00001))\n",
    "10**-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'colab_base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n colab_base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(size=(16,2))\n",
    "w = torch.zeros(size=(16,2,50))\n",
    "b = torch.zeros(size=(16,50))\n",
    "\n",
    "x = torch.unsqueeze(x,dim=1)\n",
    "g = torch.matmul(x,w)\n",
    "g = torch.squeeze(g, dim=1)\n",
    "print('g',g.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_enf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
